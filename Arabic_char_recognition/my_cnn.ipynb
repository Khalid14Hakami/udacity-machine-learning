{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tflearn.data_utils as du\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('./data/csvTrainImages 13440x1024.csv', header = None)\n",
    "y_train = pd.read_csv('./data/csvTrainLabel 13440x1.csv', header = None)\n",
    "x_test = pd.read_csv('./data/csvTestImages 3360x1024.csv', header = None)\n",
    "y_test = pd.read_csv('./data/csvTestLabel 3360x1.csv', header = None)\n",
    "\n",
    "x_train = x_train.iloc[:,:].values.astype('float32')\n",
    "y_train = y_train.iloc[:,:].values.astype('int32')-1\n",
    "x_test = x_test.iloc[:,:].values.astype('float32')\n",
    "y_test = y_test.iloc[:,:].values.astype('int32')-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 32, 32)\n",
    "x_train = x_train.swapaxes(1, 2)\n",
    "x_test = x_test.reshape(x_test.shape[0], 32, 32)\n",
    "x_test = x_test.swapaxes(1, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(x_train[2].squeeze(),cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "print(y_train[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshping data for model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this before fitting model 1\n",
    "\n",
    "\n",
    "# x_train = x_train.reshape([-1, 32, 32, 1])\n",
    "# x_test = x_test.reshape([-1, 32, 32, 1])\n",
    "\n",
    "# x_train, mean1 = du.featurewise_zero_center(x_train)\n",
    "# x_test, mean2 = du.featurewise_zero_center(x_test)\n",
    "\n",
    "# y_train = du.to_categorical(y_train,28)\n",
    "# y_test = du.to_categorical(y_test,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "\n",
    "plt.imshow(x_train[2],cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building  model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recognizer = Sequential()\n",
    "\n",
    "# recognizer.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "#                  activation ='relu', input_shape = (32,32,1)))\n",
    "# recognizer.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "#                  activation ='relu'))\n",
    "# recognizer.add(MaxPool2D(pool_size=(2,2)))\n",
    "# recognizer.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "# recognizer.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "#                  activation ='relu'))\n",
    "# recognizer.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "#                  activation ='relu'))\n",
    "# recognizer.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "# recognizer.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "# recognizer.add(Flatten())\n",
    "# recognizer.add(Dense(units = 256, input_dim = 1024, activation = 'relu'))\n",
    "# recognizer.add(Dense(units = 256, activation = \"relu\"))\n",
    "# recognizer.add(Dropout(0.5))\n",
    "# recognizer.add(Dense(28, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# recognizer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "# recognizer.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# datagen = ImageDataGenerator(\n",
    "#         featurewise_center=False, \n",
    "#         samplewise_center=False,  \n",
    "#         featurewise_std_normalization=False,\n",
    "#         samplewise_std_normalization=False,\n",
    "#         zca_whitening=False,\n",
    "#         rotation_range=10,\n",
    "#         zoom_range = 0.1,  \n",
    "#         width_shift_range=0.1, \n",
    "#         height_shift_range=0.1,\n",
    "#         horizontal_flip=False,\n",
    "#         vertical_flip=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fitting model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datagen.fit(x_train)\n",
    "# recognizer.fit_generator(datagen.flow(x_train, y_train, batch_size=100),\n",
    "#                              epochs = 30, verbose = 2, steps_per_epoch=x_train.shape[0] // 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fitting extra model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "trainx = pd.read_csv(\"./data/csvTrainImages 13440x1024.csv\",header=None)\n",
    "trainy = pd.read_csv(\"./data/csvTrainLabel 13440x1.csv\",header=None)\n",
    "\n",
    "testx = pd.read_csv(\"./data/csvTestImages 3360x1024.csv\",header=None)\n",
    "testy = pd.read_csv(\"./data/csvTestLabel 3360x1.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training set and validation set\n",
    "#training images\n",
    "trainx = trainx.values.astype('float32')\n",
    "#training labels\n",
    "trainy = trainy.values.astype('int32')-1\n",
    "\n",
    "#testing images\n",
    "testx = testx.values.astype('float32')\n",
    "#testing labels\n",
    "testy = testy.values.astype('int32')-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "original_trainy = trainy\n",
    "#One Hot encoding of train labels.\n",
    "trainy = to_categorical(trainy)\n",
    "\n",
    "original_testy = testy\n",
    "#One Hot encoding of test labels.\n",
    "testy = to_categorical(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13440, 1024) (13440, 28) (3360, 1024) (3360, 28)\n"
     ]
    }
   ],
   "source": [
    "print(trainx.shape, trainy.shape, testx.shape, testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13440, 32, 32, 1) (13440, 28) (3360, 32, 32, 1) (3360, 28)\n"
     ]
    }
   ],
   "source": [
    "# reshape input images to 28x28x1\n",
    "trainx = trainx.reshape([-1, 32, 32, 1])\n",
    "testx = testx.reshape([-1, 32, 32, 1])\n",
    "print(trainx.shape, trainy.shape, testx.shape, testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()#add model layers\n",
    "model.add(Conv2D(64, kernel_size=4, activation= 'relu', input_shape=(32,32,1)))\n",
    "model.add(Conv2D(32, kernel_size=4, activation= 'relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(28, activation= 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model using accuracy to measure model performance\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_5 to have 2 dimensions, but got array with shape (3360, 32, 32, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-2ce37f78fffa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps_per_epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m         validation_split=validation_split)\n\u001b[0m\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    915\u001b[0m           \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m           exception_prefix='target')\n\u001b[0m\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    180\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_5 to have 2 dimensions, but got array with shape (3360, 32, 32, 1)"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "model.fit(trainx, testx, validation_split=0.2, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3360/3360 [==============================] - 6s 2ms/step\n",
      "Test accuarcy: 1215.21%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testx, testy)\n",
    "print('Test accuarcy: %0.2f%%' % (score[0] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "\n",
    "# Building Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "                \n",
    "model = tf.keras.models.Sequential()\n",
    "# model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Conv2D(128, kernel_size=3,input_shape = (32,32,1) ,activation=tf.nn.relu, kernel_regularizer=regularizers.L1L2(0.001)))\n",
    "model.add(tf.keras.layers.Dropout(0.30))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu, kernel_regularizer=regularizers.L1L2(0.001)))\n",
    "model.add(tf.keras.layers.Dense(28, activation=tf.nn.softmax))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.build?\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_129 to have 4 dimensions, but got array with shape (13440, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps_per_epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m         validation_split=validation_split)\n\u001b[0m\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    915\u001b[0m           \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m           exception_prefix='target')\n\u001b[0m\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    180\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_129 to have 4 dimensions, but got array with shape (13440, 1)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from livelossplot import PlotLossesKeras\n",
    "model.fit(x_train, y_train, epochs=30, \n",
    "          callbacks=[PlotLossesKeras()],\n",
    "          verbose=1,\n",
    "         validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3360/3360 [==============================] - 0s 46us/step\n",
      "0.9798765557152884\n",
      "0.7610119047619047\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
    "print(val_loss)\n",
    "print(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmer : Dev Bishnoi\n",
    "\n",
    "# This module is constructing a deep neural network with convolutional and fully connected layers.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "width = 28\n",
    "height = 28\n",
    "N = 36 # No of classes\n",
    "flatten_size = width * height\n",
    "rate = 4e-4\n",
    "\n",
    "def model(intput_x, input_y):\n",
    "\n",
    "\twith tf.name_scope(\"reshapeTo4d\"):\n",
    "\t\treshaped = tf.reshape(intput_x, [-1, width, height, 1], name = 'reshaped')\n",
    "\n",
    "\twith tf.name_scope(\"paramsAtL1\"):\n",
    "\t\tlyr1_w = tf.Variable(tf.truncated_normal(shape = [5, 5, 1, 64], dtype = tf.float32), name = \"lyr1_w\")\n",
    "\t\tlyr1_b = tf.Variable(tf.constant(0.1, shape=[64], dtype = tf.float32), name = \"lyr1_b\")\n",
    "\n",
    "\twith tf.name_scope(\"Layer1\"):\n",
    "\t\tlyr1_opt = tf.nn.relu(tf.nn.conv2d(reshaped, lyr1_w, strides = [1, 1, 1, 1], padding = 'SAME') + lyr1_b, name = \"reluLyr1\")\n",
    "\n",
    "\twith tf.name_scope(\"maxpoolingAtLayer1\"):\n",
    "\t\tlyr1_mxopt = tf.nn.max_pool(lyr1_opt, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME', name = 'mxpoolLyr1')\n",
    "\n",
    "\twith tf.name_scope(\"paramsAtL2\"):\n",
    "\t\tlyr2_w = tf.Variable(tf.truncated_normal(shape = [5, 5, 64, 128], dtype = tf.float32), name = \"lyr2_w\")\n",
    "\t\tlyr2_b = tf.Variable(tf.constant(0.1, shape=[128], dtype = tf.float32), name = \"lyr2_b\")\n",
    "\n",
    "\twith tf.name_scope(\"Layer2\"):\n",
    "\t\tlyr2_opt = tf.nn.relu(tf.nn.conv2d(lyr1_mxopt, lyr2_w, strides = [1, 1, 1, 1], padding = 'SAME') + lyr2_b, name = \"reluLyr2\")\n",
    "\n",
    "\twith tf.name_scope(\"maxpoolingAtLayer2\"):\n",
    "\t\tlyr2_mxopt = tf.nn.max_pool(lyr2_opt, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME', name = \"mxpoolLyr2\")\n",
    "\n",
    "\twith tf.name_scope(\"FlattenForFCL\"):\n",
    "\t\tflatten_opt = tf.reshape(lyr2_mxopt, [-1, 7 * 7 * 128], name = \"Flatten\")\n",
    "\n",
    "\twith tf.name_scope(\"paramsAtL3\"):\n",
    "\t\tlyr3_w = tf.Variable(tf.truncated_normal(shape = [7 * 7 * 128, 864], dtype = tf.float32), name = \"lyr3_w\")\n",
    "\t\tlyr3_b = tf.Variable(tf.constant(0.1, shape = [864], dtype = tf.float32), name = \"lyr3_b\")\n",
    "\n",
    "\twith tf.name_scope(\"Layer3\"):\n",
    "\t\tlyr3_opt = tf.nn.relu(tf.add(tf.matmul(flatten_opt, lyr3_w), lyr3_b), name = \"reluLyr3\")\n",
    "\n",
    "\twith tf.name_scope(\"paramsAtL4\"):\n",
    "\t\tlyr4_w = tf.Variable(tf.truncated_normal(shape = [864, 432], dtype = tf.float32), name = \"lyr4_w\")\n",
    "\t\tlyr4_b = tf.Variable(tf.constant(0.1, shape = [432], dtype = tf.float32), name = \"lyr4_b\")\n",
    "\n",
    "\twith tf.name_scope(\"Layer4\"):\n",
    "\t\tlyr4_opt = tf.nn.relu(tf.add(tf.matmul(lyr3_opt, lyr4_w), lyr4_b), name = \"reluLyr4\")\n",
    "\n",
    "\twith tf.name_scope(\"paramsAtOptLayer\"):\n",
    "\t\topt_w = tf.Variable(tf.truncated_normal(shape = [432, N], dtype = tf.float32), name = \"opt_w\")\n",
    "\t\topt_b = tf.Variable(tf.constant(0.1, shape = [N], dtype = tf.float32), name = \"opt_b\")\n",
    "\n",
    "\twith tf.name_scope(\"optLayer\"):\n",
    "\t\tresultOp = tf.add(tf.matmul(lyr4_opt, opt_w), opt_b, name = \"resultOp\")\n",
    "\n",
    "\twith tf.name_scope(\"crossEntropy\"):\n",
    "\t\tcrossEntropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = resultOp, labels = input_y), name = \"crossEntropy\")\n",
    "\n",
    "\twith tf.name_scope(\"optimizer\"):\n",
    "\t\toptimizer = tf.train.AdamOptimizer(rate).minimize(crossEntropy)\n",
    "\n",
    "\twith tf.name_scope(\"truthTableBool\"):\n",
    "\t\ttruthTableBool = tf.equal(tf.argmax(resultOp, 1), tf.argmax(input_y, 1), name = \"truthTableBool\")\n",
    "\n",
    "\twith tf.name_scope(\"truthTableInt\"):\n",
    "\t\ttruthTableInt = tf.cast(truthTableBool, tf.int32, name = \"truthTableInt\")\n",
    "\n",
    "\twith tf.name_scope(\"prediction\"):\n",
    "\t\tprediction = tf.reduce_sum(truthTableInt, name = \"prediction\")\n",
    "\n",
    "\treturn crossEntropy, optimizer, prediction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension size must be evenly divisible by 784 but is 13762560 for 'reshapeTo4d_1/reshaped' (op: 'Reshape') with input shapes: [13440,32,32], [4] and with input tensors computed as partial shapes: input[1] = [?,28,28,1].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1575\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1576\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1577\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension size must be evenly divisible by 784 but is 13762560 for 'reshapeTo4d_1/reshaped' (op: 'Reshape') with input shapes: [13440,32,32], [4] and with input tensors computed as partial shapes: input[1] = [?,28,28,1].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-b5832115483b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-197-032b861a3e33>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(intput_x, input_y)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reshapeTo4d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0mreshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintput_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'reshaped'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"paramsAtL1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6198\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 6199\u001b[0;31m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[1;32m   6200\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 instructions)\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    456\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3153\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3154\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3155\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3156\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1729\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1730\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1731\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1577\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension size must be evenly divisible by 784 but is 13762560 for 'reshapeTo4d_1/reshaped' (op: 'Reshape') with input shapes: [13440,32,32], [4] and with input tensors computed as partial shapes: input[1] = [?,28,28,1]."
     ]
    }
   ],
   "source": [
    "model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
